# Prediction Market Research Pipeline
"""
Stage 5: Aggregation & Scoring
Combines quant signals + LLM output into a final probability and bet recommendation.
"""


# Weights for combining signals into final probability
WEIGHTS = {
    "llm": 0.35,
    "expert_forecast": 0.25,
    "news_sentiment": 0.15,
    "reddit_sentiment": 0.10,
    "sports_odds": 0.15,
}

# Minimum edge required to flag as an opportunity
MIN_EDGE_THRESHOLD = 0.05

# Kelly fraction (use fractional Kelly to reduce variance)
KELLY_FRACTION = 0.25

# Max bet as fraction of bankroll
MAX_BET_FRACTION = 0.05


def aggregate_and_score(market: dict, quant_signals: dict, llm_result: dict) -> dict:
    """
    Combine all signals into a final probability estimate and bet recommendation.
    """
    llm_prob = llm_result.get("probability", 0.5)
    market_price = market["market_price"]

    # --- Weighted probability blend ---
    components = {"llm": (llm_prob, WEIGHTS["llm"])}
    total_weight = WEIGHTS["llm"]

    if "expert_forecast_mean" in quant_signals:
        components["expert_forecast"] = (quant_signals["expert_forecast_mean"], WEIGHTS["expert_forecast"])
        total_weight += WEIGHTS["expert_forecast"]

    if "news_sentiment" in quant_signals:
        components["news_sentiment"] = (quant_signals["news_sentiment"], WEIGHTS["news_sentiment"])
        total_weight += WEIGHTS["news_sentiment"]

    if "reddit_sentiment" in quant_signals:
        components["reddit_sentiment"] = (quant_signals["reddit_sentiment"], WEIGHTS["reddit_sentiment"])
        total_weight += WEIGHTS["reddit_sentiment"]

    if "sports_odds_implied_prob" in quant_signals:
        components["sports_odds"] = (quant_signals["sports_odds_implied_prob"], WEIGHTS["sports_odds"])
        total_weight += WEIGHTS["sports_odds"]

    # Normalize weights and compute weighted average
    final_prob = sum(prob * (weight / total_weight) for prob, weight in components.values())
    final_prob = round(max(0.01, min(0.99, final_prob)), 4)

    # --- Edge calculation ---
    edge = final_prob - market_price

    # --- Confidence adjustment ---
    confidence = llm_result.get("confidence", "low")
    data_quality = llm_result.get("data_quality", "poor")

    # Downgrade confidence if data is poor
    if data_quality == "poor" and confidence == "high":
        confidence = "medium"

    # --- Kelly Criterion for bet sizing ---
    bet_direction = "YES" if edge > 0 else "NO"
    if bet_direction == "YES":
        p = final_prob
        q = 1 - final_prob
        b = (1 / market_price) - 1  # net odds on YES bet
    else:
        p = 1 - final_prob
        q = final_prob
        b = (1 / (1 - market_price)) - 1  # net odds on NO bet

    kelly_full = (b * p - q) / b if b > 0 else 0
    kelly_bet = max(0, KELLY_FRACTION * kelly_full)
    kelly_bet = min(kelly_bet, MAX_BET_FRACTION)  # cap at 5% of bankroll

    # Zero out bet size if confidence is low or edge is insufficient
    if confidence == "low" or abs(edge) < MIN_EDGE_THRESHOLD:
        recommended_bet_fraction = 0.0
        flag = "skip"
    else:
        recommended_bet_fraction = round(kelly_bet, 4)
        flag = "opportunity" if abs(edge) >= MIN_EDGE_THRESHOLD else "skip"

    return {
        "final_prob": final_prob,
        "market_price": market_price,
        "edge": round(edge, 4),
        "confidence": confidence,
        "data_quality": data_quality,
        "bet_direction": bet_direction,
        "recommended_bet_fraction": recommended_bet_fraction,  # fraction of bankroll
        "flag": flag,
        "components": {k: round(v[0], 4) for k, v in components.items()},
        "market_assessment": llm_result.get("market_assessment", "fairly_priced"),
        "key_factors": llm_result.get("key_factors", []),
        "reasoning": llm_result.get("reasoning", ""),
        "bull_case": llm_result.get("bull_case", ""),
        "bear_case": llm_result.get("bear_case", ""),
    }


def kelly_bet_size(bankroll: float, final: dict) -> float:
    """
    Convert fractional Kelly to a dollar amount.
    Usage: bet_usd = kelly_bet_size(my_bankroll, final_result)
    """
    return round(bankroll * final["recommended_bet_fraction"], 2)

"""
Stage 2: Data Collection
Fetches relevant raw data for each market based on its category.
"""

import aiohttp
import os
import asyncio
from datetime import datetime, timedelta


async def collect_signals(market: dict) -> dict:
    """
    Collect raw data for a market. Returns dict of source -> data.
    """
    category = market["category"]
    question = market["question"]

    tasks = {
        "news": _fetch_news(question),
        "metaculus": _fetch_metaculus_forecasts(question),
    }

    # Category-specific sources
    if category == "politics":
        tasks["reddit"] = _fetch_reddit(question, subreddits=["politics", "PoliticalDiscussion", "worldnews"])

    elif category == "crypto_finance":
        tasks["crypto_prices"] = _fetch_crypto_signals(question)
        tasks["reddit"] = _fetch_reddit(question, subreddits=["cryptocurrency", "investing", "wallstreetbets"])

    elif category == "sports":
        tasks["odds"] = _fetch_sports_odds(question)
        tasks["reddit"] = _fetch_reddit(question, subreddits=["sports", "nfl", "nba", "soccer"])

    elif category == "current_events":
        tasks["reddit"] = _fetch_reddit(question, subreddits=["worldnews", "news"])

    # Run all tasks concurrently
    results = await asyncio.gather(*tasks.values(), return_exceptions=True)
    raw_data = {}
    for key, result in zip(tasks.keys(), results):
        if isinstance(result, Exception):
            print(f"  Warning: {key} collection failed: {result}")
            raw_data[key] = None
        else:
            raw_data[key] = result

    return raw_data


async def _fetch_news(query: str, days_back: int = 7) -> dict:
    """Fetch recent news articles via NewsAPI."""
    api_key = os.getenv("NEWSAPI_KEY")
    if not api_key:
        return {"articles": [], "error": "NEWSAPI_KEY not set"}

    from_date = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")

    async with aiohttp.ClientSession() as session:
        url = "https://newsapi.org/v2/everything"
        params = {
            "q": _extract_keywords(query),
            "from": from_date,
            "sortBy": "relevancy",
            "language": "en",
            "pageSize": 10,
            "apiKey": api_key,
        }
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as resp:
            if resp.status != 200:
                return {"articles": [], "error": f"HTTP {resp.status}"}
            data = await resp.json()

    articles = []
    for a in data.get("articles", []):
        articles.append({
            "title": a.get("title", ""),
            "description": a.get("description", ""),
            "source": a.get("source", {}).get("name", ""),
            "published_at": a.get("publishedAt", ""),
            "url": a.get("url", ""),
        })

    return {"articles": articles, "total_results": data.get("totalResults", 0)}


async def _fetch_metaculus_forecasts(query: str) -> dict:
    """Search Metaculus for similar questions and their community forecasts."""
    async with aiohttp.ClientSession() as session:
        url = "https://www.metaculus.com/api2/questions/"
        params = {
            "search": _extract_keywords(query)[:100],
            "status": "open",
            "order_by": "-activity",
            "limit": 5,
        }
        headers = {"User-Agent": "PredictionBot/1.0"}
        async with session.get(url, params=params, headers=headers,
                               timeout=aiohttp.ClientTimeout(total=10)) as resp:
            if resp.status != 200:
                return {"questions": []}
            data = await resp.json()

    questions = []
    for q in data.get("results", []):
        community = q.get("community_prediction", {})
        forecast = community.get("full", {}).get("q2")  # median forecast
        questions.append({
            "title": q.get("title", ""),
            "community_forecast": forecast,
            "resolution_criteria": q.get("resolution_criteria", "")[:300],
            "url": f"https://metaculus.com{q.get('page_url', '')}",
        })

    return {"questions": questions}


async def _fetch_reddit(query: str, subreddits: list[str]) -> dict:
    """Fetch recent Reddit posts for sentiment."""
    # Using Reddit's public JSON API (no auth needed for read)
    all_posts = []
    keywords = _extract_keywords(query)

    async with aiohttp.ClientSession() as session:
        headers = {"User-Agent": "PredictionBot/1.0"}
        for sub in subreddits[:2]:  # limit to avoid rate limiting
            url = f"https://www.reddit.com/r/{sub}/search.json"
            params = {"q": keywords[:100], "sort": "new", "limit": 10, "t": "week"}
            try:
                async with session.get(url, params=params, headers=headers,
                                       timeout=aiohttp.ClientTimeout(total=8)) as resp:
                    if resp.status != 200:
                        continue
                    data = await resp.json()
                    for post in data.get("data", {}).get("children", []):
                        p = post.get("data", {})
                        all_posts.append({
                            "title": p.get("title", ""),
                            "score": p.get("score", 0),
                            "num_comments": p.get("num_comments", 0),
                            "subreddit": p.get("subreddit", ""),
                            "created_utc": p.get("created_utc", 0),
                        })
            except Exception:
                continue
            await asyncio.sleep(0.5)  # be polite

    return {"posts": all_posts, "subreddits_searched": subreddits}


async def _fetch_crypto_signals(query: str) -> dict:
    """Fetch crypto price data from CoinGecko (free tier)."""
    # Map common coins mentioned in question
    coin_map = {
        "bitcoin": "bitcoin", "btc": "bitcoin",
        "ethereum": "ethereum", "eth": "ethereum",
        "solana": "solana", "sol": "solana",
    }

    q_lower = query.lower()
    coins_to_fetch = [cg_id for keyword, cg_id in coin_map.items() if keyword in q_lower]
    if not coins_to_fetch:
        coins_to_fetch = ["bitcoin"]  # default

    prices = {}
    async with aiohttp.ClientSession() as session:
        ids = ",".join(set(coins_to_fetch))
        url = "https://api.coingecko.com/api/v3/simple/price"
        params = {"ids": ids, "vs_currencies": "usd",
                  "include_24hr_change": "true", "include_7d_change": "true"}
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as resp:
            if resp.status == 200:
                prices = await resp.json()

    return {"prices": prices}


async def _fetch_sports_odds(query: str) -> dict:
    """Fetch sports betting odds from The Odds API."""
    api_key = os.getenv("ODDS_API_KEY")
    if not api_key:
        return {"odds": [], "error": "ODDS_API_KEY not set"}

    # Map to sport keys
    sport_map = {
        "nfl": "americanfootball_nfl",
        "nba": "basketball_nba",
        "mlb": "baseball_mlb",
        "nhl": "icehockey_nhl",
        "soccer": "soccer_epl",
    }
    q_lower = query.lower()
    sport_key = next((v for k, v in sport_map.items() if k in q_lower), "americanfootball_nfl")

    async with aiohttp.ClientSession() as session:
        url = f"https://api.the-odds-api.com/v4/sports/{sport_key}/odds"
        params = {
            "apiKey": api_key,
            "regions": "us",
            "markets": "h2h",
            "oddsFormat": "decimal",
        }
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as resp:
            if resp.status != 200:
                return {"odds": []}
            data = await resp.json()

    return {"odds": data[:5], "sport": sport_key}  # top 5 games


def _extract_keywords(question: str, max_words: int = 8) -> str:
    """Extract key search terms from a market question."""
    stop_words = {"will", "the", "a", "an", "be", "is", "are", "was", "were",
                  "to", "of", "in", "on", "at", "by", "for", "with", "or", "and",
                  "win", "lose", "before", "after", "by", "than", "this", "that"}
    words = [w.strip("?.,!\"'") for w in question.split()]
    keywords = [w for w in words if w.lower() not in stop_words and len(w) > 2]
    return " ".join(keywords[:max_words])

"""
Stage 4: LLM Research & Reasoning
Uses Claude to synthesize signals and reason about market probability.
"""

import json
import os
import anthropic
from pipeline.quantitative import format_signals_for_prompt


client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))


def _summarize_news(raw_data: dict, max_articles: int = 8) -> str:
    """Build a concise news summary string to include in the prompt."""
    news = raw_data.get("news", {})
    if not news or not news.get("articles"):
        return "No news articles found."

    lines = []
    for i, a in enumerate(news["articles"][:max_articles]):
        title = a.get("title", "").strip()
        desc = a.get("description", "").strip()
        source = a.get("source", "")
        pub = a.get("published_at", "")[:10]
        if title:
            lines.append(f"[{i+1}] ({source}, {pub}) {title}")
            if desc:
                lines.append(f"    {desc[:200]}")
    return "\n".join(lines) if lines else "No articles available."


def _summarize_metaculus(raw_data: dict) -> str:
    """Summarize Metaculus crowd forecasts."""
    metaculus = raw_data.get("metaculus", {})
    questions = metaculus.get("questions", []) if metaculus else []
    if not questions:
        return "No Metaculus forecasts found."

    lines = []
    for q in questions[:3]:
        fc = q.get("community_forecast")
        title = q.get("title", "")[:100]
        lines.append(f"  - \"{title}\" → forecast: {fc:.0%}" if fc else f"  - \"{title}\" → no forecast")
    return "\n".join(lines)


async def run_llm_research(market: dict, quant_signals: dict, raw_data: dict) -> dict:
    """
    Send market + signals to Claude for superforecaster-style reasoning.
    Returns probability estimate, confidence, reasoning, and edge.
    """
    news_summary = _summarize_news(raw_data)
    metaculus_summary = _summarize_metaculus(raw_data)
    signals_text = format_signals_for_prompt(quant_signals)

    prompt = f"""You are an expert superforecaster analyzing a prediction market question.
Your job is to estimate the probability that the following resolves YES.

MARKET QUESTION:
{market['question']}

RESOLUTION DATE: {market['close_date'][:10]} ({market['days_to_close']} days from now)
MARKET URL: {market['url']}

--- QUANTITATIVE SIGNALS ---
{signals_text}

--- SIMILAR METACULUS FORECASTS (crowd wisdom) ---
{metaculus_summary}

--- RECENT NEWS (do not assume facts beyond what is shown here) ---
{news_summary}

--- YOUR TASK ---
Reason step by step like a careful superforecaster. Consider:
1. What are the key factors determining this outcome?
2. What base rate applies to questions like this?
3. What do the quantitative signals suggest?
4. Does the news provide meaningful new information, or is it noise?
5. Is the current market price reasonable, too high, or too low?
6. What would need to happen for this to resolve YES vs NO?

Then give your final probability estimate.

Respond ONLY with a JSON object in this exact format:
{{
  "probability": <float between 0.0 and 1.0>,
  "confidence": "<low|medium|high>",
  "key_factors": ["<factor 1>", "<factor 2>", "<factor 3>"],
  "bull_case": "<1-2 sentences on why it resolves YES>",
  "bear_case": "<1-2 sentences on why it resolves NO>",
  "reasoning": "<3-5 sentence summary of your analysis>",
  "data_quality": "<poor|fair|good> - how good was the data available?",
  "market_assessment": "<underpriced|fairly_priced|overpriced>"
}}"""

    response = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,  # lower = more consistent reasoning
    )

    raw_text = response.content[0].text.strip()

    # Strip markdown code fences if present
    if raw_text.startswith("```"):
        raw_text = raw_text.split("```")[1]
        if raw_text.startswith("json"):
            raw_text = raw_text[4:]
    raw_text = raw_text.strip()

    try:
        result = json.loads(raw_text)
    except json.JSONDecodeError:
        # Fallback: extract probability if JSON parse fails
        import re
        prob_match = re.search(r'"probability"\s*:\s*([\d.]+)', raw_text)
        prob = float(prob_match.group(1)) if prob_match else 0.5
        result = {
            "probability": prob,
            "confidence": "low",
            "reasoning": raw_text[:500],
            "key_factors": [],
            "bull_case": "",
            "bear_case": "",
            "data_quality": "poor",
            "market_assessment": "fairly_priced",
        }

    # Clamp probability to valid range
    result["probability"] = max(0.01, min(0.99, float(result.get("probability", 0.5))))

    return result

"""
Prediction Market Research Pipeline
Runs daily to find and score market opportunities.
"""

import asyncio
import logging
from datetime import datetime
from pipeline.market_ingestion import fetch_open_markets
from pipeline.data_collection import collect_signals
from pipeline.quantitative import compute_quant_signals
from pipeline.llm_research import run_llm_research
from pipeline.aggregator import aggregate_and_score
from pipeline.storage import Database

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger(__name__)


async def run_pipeline():
    log.info("=== Starting Daily Research Pipeline ===")
    db = Database()

    # Stage 1: Pull open markets
    log.info("Stage 1: Fetching open markets...")
    markets = await fetch_open_markets()
    log.info(f"  Found {len(markets)} markets to research")

    results = []
    for i, market in enumerate(markets):
        log.info(f"  [{i+1}/{len(markets)}] Researching: {market['question'][:60]}...")

        try:
            # Stage 2: Collect raw data
            raw_data = await collect_signals(market)

            # Stage 3: Quantitative signals
            quant = compute_quant_signals(market, raw_data)

            # Stage 4: LLM reasoning
            llm_result = await run_llm_research(market, quant, raw_data)

            # Stage 5: Final score
            final = aggregate_and_score(market, quant, llm_result)

            # Store result
            db.save_research(market, quant, llm_result, final)
            results.append({"market": market, "final": final})

        except Exception as e:
            log.error(f"  Error processing market {market['id']}: {e}")
            continue

    # Print daily summary
    print_summary(results)
    log.info("=== Pipeline Complete ===")
    return results


def print_summary(results):
    opportunities = [r for r in results if abs(r["final"]["edge"]) > 0.05
                     and r["final"]["confidence"] in ("medium", "high")]
    opportunities.sort(key=lambda r: abs(r["final"]["edge"]), reverse=True)

    print("\n" + "="*60)
    print(f"DAILY RESEARCH SUMMARY — {datetime.now().strftime('%Y-%m-%d')}")
    print("="*60)
    print(f"Markets researched: {len(results)}")
    print(f"Opportunities found (edge > 5%): {len(opportunities)}\n")

    if opportunities:
        print("TOP OPPORTUNITIES:")
        for r in opportunities[:10]:
            m = r["market"]
            f = r["final"]
            direction = "YES" if f["final_prob"] > m["market_price"] else "NO"
            print(f"  [{m['category'].upper()}] {m['question'][:55]}...")
            print(f"    Market: {m['market_price']:.0%} | Our estimate: {f['final_prob']:.0%} | "
                  f"Edge: {f['edge']:+.0%} | Bet: {direction} | Confidence: {f['confidence']}")
            print()


if __name__ == "__main__":
    asyncio.run(run_pipeline())

"""
Stage 3: Quantitative Pre-processing
Computes structured signals from raw data before sending to the LLM.
"""

from datetime import datetime
import math


def compute_quant_signals(market: dict, raw_data: dict) -> dict:
    """
    Transform raw data into normalized quantitative signals.
    All scores are in [0, 1] range representing probability of YES.
    """
    signals = {
        "market_price": market["market_price"],
        "days_to_close": market["days_to_close"],
        "volume": market["volume"],
    }

    # --- Metaculus crowd forecast ---
    metaculus_data = raw_data.get("metaculus")
    if metaculus_data and metaculus_data.get("questions"):
        forecasts = [q["community_forecast"] for q in metaculus_data["questions"]
                     if q.get("community_forecast") is not None]
        if forecasts:
            signals["expert_forecast_mean"] = sum(forecasts) / len(forecasts)
            signals["expert_forecast_count"] = len(forecasts)

    # --- News sentiment ---
    news_data = raw_data.get("news")
    if news_data and news_data.get("articles"):
        sentiment = _score_news_sentiment(news_data["articles"], market["question"])
        signals["news_sentiment"] = sentiment
        signals["news_article_count"] = len(news_data["articles"])

    # --- Reddit sentiment ---
    reddit_data = raw_data.get("reddit")
    if reddit_data and reddit_data.get("posts"):
        sentiment = _score_reddit_sentiment(reddit_data["posts"], market["question"])
        signals["reddit_sentiment"] = sentiment
        signals["reddit_post_count"] = len(reddit_data["posts"])

    # --- Crypto-specific signals ---
    crypto_data = raw_data.get("crypto_prices")
    if crypto_data and crypto_data.get("prices"):
        signals["crypto_signals"] = _parse_crypto_signals(crypto_data["prices"])

    # --- Sports odds ---
    odds_data = raw_data.get("odds")
    if odds_data and odds_data.get("odds"):
        implied_prob = _parse_sports_odds(odds_data["odds"], market["question"])
        if implied_prob is not None:
            signals["sports_odds_implied_prob"] = implied_prob

    # --- Urgency signal (time decay) ---
    signals["urgency_score"] = _urgency_score(market["days_to_close"])

    return signals


def _score_news_sentiment(articles: list, question: str) -> float:
    """
    Simple keyword-based sentiment scoring.
    Returns probability leaning (0=strongly NO, 1=strongly YES).
    """
    # Keywords that suggest YES resolution
    positive_words = {
        "wins", "won", "victory", "passes", "approved", "rises", "surges", "beats",
        "confirms", "elected", "leads", "ahead", "likely", "expected", "probable",
        "increased", "higher", "above", "breakthrough", "success"
    }
    # Keywords that suggest NO resolution
    negative_words = {
        "loses", "lost", "defeat", "fails", "rejected", "drops", "falls", "trails",
        "unlikely", "unexpected", "below", "missed", "declined", "lower", "collapse",
        "suspended", "cancelled", "postponed", "reversed"
    }

    pos_count = 0
    neg_count = 0
    for article in articles:
        text = (article.get("title", "") + " " + article.get("description", "")).lower()
        pos_count += sum(1 for w in positive_words if w in text)
        neg_count += sum(1 for w in negative_words if w in text)

    total = pos_count + neg_count
    if total == 0:
        return 0.5  # neutral

    # Sigmoid-like normalization
    raw_score = pos_count / total
    # Pull toward 0.5 based on sample size (low confidence with few signals)
    weight = min(1.0, total / 10)
    return 0.5 + (raw_score - 0.5) * weight


def _score_reddit_sentiment(posts: list, question: str) -> float:
    """Score Reddit posts for sentiment, weighted by upvotes."""
    positive_words = {"yes", "will", "going to", "definitely", "confirmed", "likely", "bullish"}
    negative_words = {"no", "won't", "not", "unlikely", "bearish", "fail", "doubt"}

    weighted_pos = 0
    weighted_neg = 0

    for post in posts:
        title = post.get("title", "").lower()
        score = max(1, post.get("score", 1))  # weight by upvotes (floor at 1)
        log_score = math.log(score + 1)

        pos = sum(1 for w in positive_words if w in title)
        neg = sum(1 for w in negative_words if w in title)
        weighted_pos += pos * log_score
        weighted_neg += neg * log_score

    total = weighted_pos + weighted_neg
    if total == 0:
        return 0.5
    return weighted_pos / total


def _parse_crypto_signals(prices: dict) -> dict:
    """Extract directional signals from crypto price data."""
    signals = {}
    for coin, data in prices.items():
        change_24h = data.get("usd_24h_change", 0) or 0
        change_7d = data.get("usd_7d_change", 0) or 0
        # Convert % changes to directional probability
        signals[coin] = {
            "price_usd": data.get("usd"),
            "change_24h_pct": change_24h,
            "change_7d_pct": change_7d,
            "momentum": _pct_to_signal(change_7d),
        }
    return signals


def _parse_sports_odds(odds_list: list, question: str) -> float | None:
    """Try to extract an implied probability from sports odds relevant to the question."""
    q_lower = question.lower()

    for game in odds_list:
        home_team = game.get("home_team", "").lower()
        away_team = game.get("away_team", "").lower()

        # Check if either team is mentioned in the question
        team_match = None
        if home_team and home_team.split()[-1] in q_lower:
            team_match = "home"
        elif away_team and away_team.split()[-1] in q_lower:
            team_match = "away"

        if team_match:
            for bookmaker in game.get("bookmakers", [])[:1]:  # use first bookmaker
                for market in bookmaker.get("markets", []):
                    if market.get("key") == "h2h":
                        outcomes = market.get("outcomes", [])
                        for outcome in outcomes:
                            # Convert decimal odds to implied probability
                            dec_odds = outcome.get("price", 2.0)
                            implied = 1 / dec_odds if dec_odds > 0 else 0.5
                            if team_match == "home" and outcome.get("name") == game.get("home_team"):
                                return implied
                            elif team_match == "away" and outcome.get("name") == game.get("away_team"):
                                return implied
    return None


def _pct_to_signal(pct_change: float) -> float:
    """Convert a percentage change to a 0-1 directional signal."""
    # Clamp to [-50%, +50%] range, map to [0, 1]
    clamped = max(-50, min(50, pct_change))
    return (clamped + 50) / 100


def _urgency_score(days_to_close: int) -> float:
    """
    Score from 0-1 representing how time-sensitive this market is.
    High score = closing soon = more urgency to act.
    """
    if days_to_close <= 0:
        return 1.0
    if days_to_close >= 30:
        return 0.0
    return 1 - (days_to_close / 30)


def format_signals_for_prompt(signals: dict) -> str:
    """Format signals dict into a readable string for the LLM prompt."""
    lines = []
    field_labels = {
        "market_price": "Current market implied probability",
        "days_to_close": "Days until market closes",
        "volume": "Total trading volume (USD)",
        "expert_forecast_mean": "Metaculus crowd forecast (mean)",
        "expert_forecast_count": "Number of similar Metaculus questions",
        "news_sentiment": "News sentiment score (0=negative, 1=positive)",
        "news_article_count": "Number of relevant news articles found",
        "reddit_sentiment": "Reddit sentiment score",
        "reddit_post_count": "Reddit posts analyzed",
        "sports_odds_implied_prob": "Sports betting implied probability",
        "urgency_score": "Urgency score (1=closing very soon)",
    }
    for key, label in field_labels.items():
        if key in signals:
            val = signals[key]
            if isinstance(val, float):
                lines.append(f"  {label}: {val:.3f}")
            else:
                lines.append(f"  {label}: {val}")

    if "crypto_signals" in signals:
        lines.append("  Crypto price signals:")
        for coin, data in signals["crypto_signals"].items():
            lines.append(f"    {coin}: ${data['price_usd']:,.0f} | "
                         f"24h: {data['change_24h_pct']:+.1f}% | "
                         f"7d: {data['change_7d_pct']:+.1f}%")

    return "\n".join(lines)

"""
Stage 6: Storage
Persists markets, research results, and bets to SQLite.
"""

import sqlite3
import json
from datetime import datetime
from pathlib import Path


DB_PATH = Path(__file__).parent.parent / "data" / "pipeline.db"


class Database:
    def __init__(self, db_path: str | None = None):
        self.db_path = db_path or str(DB_PATH)
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        self._init_schema()

    def _connect(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_schema(self):
        with self._connect() as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS markets (
                    id TEXT NOT NULL,
                    source TEXT NOT NULL,
                    question TEXT,
                    category TEXT,
                    close_date TEXT,
                    url TEXT,
                    first_seen TEXT DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (id, source)
                );

                CREATE TABLE IF NOT EXISTS research (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    market_id TEXT NOT NULL,
                    market_source TEXT NOT NULL,
                    run_date TEXT NOT NULL,
                    market_price REAL,
                    final_prob REAL,
                    edge REAL,
                    confidence TEXT,
                    flag TEXT,
                    bet_direction TEXT,
                    recommended_bet_fraction REAL,
                    quant_signals TEXT,   -- JSON
                    llm_reasoning TEXT,
                    full_result TEXT,     -- JSON
                    FOREIGN KEY (market_id, market_source) REFERENCES markets(id, source)
                );

                CREATE TABLE IF NOT EXISTS bets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    market_id TEXT NOT NULL,
                    market_source TEXT NOT NULL,
                    placed_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    direction TEXT,
                    bet_size_usd REAL,
                    market_price_at_bet REAL,
                    our_prob_at_bet REAL,
                    status TEXT DEFAULT 'open',  -- open, won, lost, cancelled
                    resolved_at TEXT,
                    pnl_usd REAL
                );

                CREATE INDEX IF NOT EXISTS idx_research_market ON research(market_id, market_source);
                CREATE INDEX IF NOT EXISTS idx_research_flag ON research(flag);
                CREATE INDEX IF NOT EXISTS idx_bets_status ON bets(status);
            """)

    def save_research(self, market: dict, quant: dict, llm_result: dict, final: dict):
        """Save a complete research result to the database."""
        with self._connect() as conn:
            # Upsert market
            conn.execute("""
                INSERT OR IGNORE INTO markets (id, source, question, category, close_date, url)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (market["id"], market["source"], market["question"],
                  market["category"], market["close_date"], market["url"]))

            # Insert research record
            conn.execute("""
                INSERT INTO research
                (market_id, market_source, run_date, market_price, final_prob, edge,
                 confidence, flag, bet_direction, recommended_bet_fraction,
                 quant_signals, llm_reasoning, full_result)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                market["id"],
                market["source"],
                datetime.utcnow().isoformat(),
                market["market_price"],
                final["final_prob"],
                final["edge"],
                final["confidence"],
                final["flag"],
                final["bet_direction"],
                final["recommended_bet_fraction"],
                json.dumps(quant),
                final.get("reasoning", ""),
                json.dumps(final),
            ))

    def record_bet(self, market_id: str, source: str, direction: str,
                   bet_size_usd: float, market_price: float, our_prob: float):
        """Record a placed bet."""
        with self._connect() as conn:
            conn.execute("""
                INSERT INTO bets
                (market_id, market_source, direction, bet_size_usd, market_price_at_bet, our_prob_at_bet)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (market_id, source, direction, bet_size_usd, market_price, our_prob))

    def resolve_bet(self, bet_id: int, won: bool, pnl_usd: float):
        """Mark a bet as resolved."""
        status = "won" if won else "lost"
        with self._connect() as conn:
            conn.execute("""
                UPDATE bets SET status=?, resolved_at=?, pnl_usd=?
                WHERE id=?
            """, (status, datetime.utcnow().isoformat(), pnl_usd, bet_id))

    def get_opportunities(self, min_edge: float = 0.05, confidence: list | None = None) -> list[dict]:
        """Fetch today's flagged opportunities."""
        confidence = confidence or ["medium", "high"]
        placeholders = ",".join("?" * len(confidence))
        today = datetime.utcnow().date().isoformat()

        with self._connect() as conn:
            rows = conn.execute(f"""
                SELECT r.*, m.question, m.category, m.url, m.close_date
                FROM research r
                JOIN markets m ON r.market_id = m.id AND r.market_source = m.source
                WHERE r.flag = 'opportunity'
                  AND ABS(r.edge) >= ?
                  AND r.confidence IN ({placeholders})
                  AND DATE(r.run_date) = ?
                ORDER BY ABS(r.edge) DESC
            """, (min_edge, *confidence, today)).fetchall()

        return [dict(row) for row in rows]

    def get_open_bets(self) -> list[dict]:
        """Fetch all open (unresolved) bets."""
        with self._connect() as conn:
            rows = conn.execute("""
                SELECT b.*, m.question, m.close_date
                FROM bets b
                JOIN markets m ON b.market_id = m.id AND b.market_source = m.source
                WHERE b.status = 'open'
                ORDER BY b.placed_at DESC
            """).fetchall()
        return [dict(row) for row in rows]

    def pnl_summary(self) -> dict:
        """Return overall P&L stats."""
        with self._connect() as conn:
            row = conn.execute("""
                SELECT
                    COUNT(*) as total_bets,
                    SUM(CASE WHEN status='won' THEN 1 ELSE 0 END) as wins,
                    SUM(CASE WHEN status='lost' THEN 1 ELSE 0 END) as losses,
                    SUM(COALESCE(pnl_usd, 0)) as total_pnl,
                    SUM(bet_size_usd) as total_wagered
                FROM bets
                WHERE status IN ('won', 'lost')
            """).fetchone()
        return dict(row) if row else {}
